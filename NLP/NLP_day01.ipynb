{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"8a5IeKq_Jquq","outputId":"bac51d21-cb96-45bc-9b94-995e19201109"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n텍스트전처리\\n-토큰(글자,단어,문장,문단)화\\n-불용어\\n-정규표현식\\n-인코딩, 패\\n-아버지, 가방\\n아버지가방에들어가신다 =>\\nkonlpy.org\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["\"\"\"\n","텍스트전처리\n","-토큰화(글자,단어,문장,문단)\n","-불용어\n","-정규표현식\n","-인코딩, 패딩\n","konlpy.org\n","\"\"\"\n"]},{"cell_type":"code","source":["\"\"\"\n","자연어처리 환경 구성\n","1. 텐서플로우 설치\n","pip install tensorflow\n","2. nltk 설치\n","pip install nltk\n","설치 후\n","import nltk\n","nltk.download()\n","3. konlpy 설치\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"Rx1TzxmvqXqd","outputId":"cc5c0bf9-63fd-4930-fb2f-01f96b2e680c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n자연어처리 환경 구성\\n1. 텐서플로우 설치\\npip install tensorflow\\n2. nltk 설치\\npip install nltk\\n설치 후\\nimport nltk\\nnltk.download()\\n3. konlpy 설치\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WkgcauEsaV3","outputId":"e9c41c72-5f15-4226-e4fe-820c2fa5069e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"]}]},{"cell_type":"code","source":["import konlpy\n","from konlpy.tag import Okt"],"metadata":{"id":"yt9CT41Ksck1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# morphs : 형태소\n","okt = Okt()\n","print(okt.morphs(u'단독입찰보다 복수입찰의 경우'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azCsc2ius6Ct","outputId":"c7d39854-a39b-49a6-d419-8f89b8e560de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n"]}]},{"cell_type":"code","source":["print(okt.morphs(u'아버지가방에들어가신다'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NBz32RpytCEG","outputId":"417044a4-c952-498a-fc48-65c6e8f3d02f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['아버지', '가방', '에', '들어가신다']\n"]}]},{"cell_type":"code","source":["\"\"\"\n","단어가방(Bag of words;BOW) : 단어들이 순서와 관계없이 저장,사전\n","코퍼스(corpus;말뭉치) : 자연어처리(분석)하고자 하는 분야와 관련된 단어 집합\n","ex) 법률서비스 챗봇-> 법률코퍼스:기소,벌금,...\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"IFbDftwZtKaG","outputId":"77d3b07d-cd52-4be6-ec0c-694ee4e72d71"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n단어가방(Bag of words;BOW)?단어들이 순서와 관계없이 저장,사전\\n코퍼스(corpus;말뭉치)?자연어처리(분석)하고자 하는 분야와 관련된 \\n단어 집합\\nex) 법률서비스 챗봇-> 법률코퍼스:기소,벌금,...\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["#nltk : 영어 코퍼스 토큰화 도구\n","import nltk\n","from nltk.tokenize import word_tokenize, WordPunctTokenizer, sent_tokenize\n","# word_tokenize : 공백과 구두점을 기준으로 분리\n","# WordPunctTokenizer : 단어와 구두점을 모두 따로 분리\n","# sent_tokenize : 문장별로 쪼개어 리스트로 반환"],"metadata":{"id":"tbvVIsmqvllr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8vQMLTavjMM","outputId":"6a48708f-ada8-4946-c2ea-d172601f5f25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["print(\"단어 토큰화 결과 : \", word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"II_k5a7-vRdk","outputId":"e3119cc3-808d-45e5-8d08-3c6cc0c798ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 토큰화 결과 :  ['Tommy', \"'s\", 'Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"]}]},{"cell_type":"code","source":["print(\"단어 토큰화 결과 : \", WordPunctTokenizer().tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JGl0jQNwveGM","outputId":"1f184d0d-3469-4621-b5e9-a904a8034617"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 토큰화 결과 :  ['Tommy', \"'\", 's', 'Don', \"'\", 't', 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"]}]},{"cell_type":"code","source":["data=\"Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\"\n"],"metadata":{"id":"VuealAxBwjV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sent_tokenize(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4op_YZnxaDN","outputId":"233199f6-b034-403f-a368-3429cbdd8faa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Language is a thing of beauty.',\n"," 'But mastering a new language from scratch is quite a daunting prospect.',\n"," 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n"," 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.']"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["# pos_tag : 품사 태깅. 각 단어에 대해 어떤 품사로 사용되는지 출력\n","from nltk.tag import pos_tag"],"metadata":{"id":"B4gM73lUyAl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBmJt9b7yZTT","outputId":"644c8a6d-1ff6-4432-9b48-9925a8ed412b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["print(\"단어 토큰화 결과 : \", word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))\n","res=word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\")\n","pos_tag(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCwZBHhJxd7L","outputId":"2b43d7fb-c719-40a7-80fb-267c2b4f032d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 토큰화 결과 :  ['Tommy', \"'s\", 'Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"]},{"output_type":"execute_result","data":{"text/plain":["[('Tommy', 'NNP'),\n"," (\"'s\", 'POS'),\n"," ('Do', 'VBP'),\n"," (\"n't\", 'RB'),\n"," ('And', 'CC'),\n"," ('that', 'IN'),\n"," ('’', 'NNP'),\n"," ('s', 'VBZ'),\n"," ('exactly', 'RB'),\n"," ('the', 'DT'),\n"," ('way', 'NN'),\n"," ('with', 'IN'),\n"," ('our', 'PRP$'),\n"," ('machines', 'NNS'),\n"," ('.', '.'),\n"," ('In', 'IN'),\n"," ('order', 'NN'),\n"," ('to', 'TO'),\n"," ('get', 'VB'),\n"," ('our', 'PRP$'),\n"," ('computer', 'NN'),\n"," ('to', 'TO'),\n"," ('understand', 'VB'),\n"," ('any', 'DT'),\n"," ('text', 'NN'),\n"," (',', ','),\n"," ('we', 'PRP'),\n"," ('need', 'VBP'),\n"," ('to', 'TO'),\n"," ('break', 'VB'),\n"," ('that', 'IN'),\n"," ('word', 'NN'),\n"," ('down', 'RP'),\n"," ('in', 'IN'),\n"," ('a', 'DT'),\n"," ('way', 'NN'),\n"," ('that', 'IN'),\n"," ('our', 'PRP$'),\n"," ('machine', 'NN'),\n"," ('can', 'MD'),\n"," ('understand', 'VB'),\n"," ('.', '.'),\n"," ('That', 'DT'),\n"," ('’', 'VBZ'),\n"," ('s', 'NN'),\n"," ('where', 'WRB'),\n"," ('the', 'DT'),\n"," ('concept', 'NN'),\n"," ('of', 'IN'),\n"," ('tokenization', 'NN'),\n"," ('in', 'IN'),\n"," ('Natural', 'NNP'),\n"," ('Language', 'NNP'),\n"," ('Processing', 'NNP'),\n"," ('(', '('),\n"," ('NLP', 'NNP'),\n"," (')', ')'),\n"," ('comes', 'VBZ'),\n"," ('in', 'IN'),\n"," ('.', '.')]"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["pip install kss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26UP5zUVDes3","outputId":"4781d818-468b-42af-9169-945200ef2b62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kss in /usr/local/lib/python3.10/dist-packages (4.5.4)\n","Requirement already satisfied: emoji==1.2.0 in /usr/local/lib/python3.10/dist-packages (from kss) (1.2.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from kss) (2022.10.31)\n","Requirement already satisfied: pecab in /usr/local/lib/python3.10/dist-packages (from kss) (1.0.8)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (1.22.4)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (9.0.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.2.2)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1.0)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.2.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.2)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n"]}]},{"cell_type":"code","source":["# kss : 한국어 문장 토큰화 도구\n","# 대문자로 시작하는 것은 클래스이므로 객체 생성필요\n","import kss\n","from konlpy.tag import Okt, Kkma"],"metadata":{"id":"zHOsGVtkF1w-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text=\"여름입니다. 날씨가 덥습니다! 딥러닝을 공부합니다. 네?\""],"metadata":{"id":"GViNrPsAF8hl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kss.split_sentences(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JbEL8E-GN4O","outputId":"5d7c2881-f096-4f3d-b5b3-87be100e590b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['여름입니다.', '날씨가 덥습니다!', '딥러닝을 공부합니다.', '네?']"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["okt=Okt()\n","kkma=Kkma()"],"metadata":{"id":"X_0bwlnaINHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Okt : \", okt.morphs(\"NLP를 열심히 공부하고, 취업에 성공합시다\"))\n","print(\"Okt : \", okt.pos(\"NLP를 열심히 공부하고, 취업에 성공합시다\"))\n","print(\"Okt : \", okt.nouns(\"NLP를 열심히 공부하고, 취업에 성공합시다\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7V0RvBPQITMv","outputId":"097266e6-f154-4ec3-9e1b-5bcbb3e6a8fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Okt :  ['NLP', '를', '열심히', '공부', '하고', ',', '취업', '에', '성공합시다']\n","Okt :  [('NLP', 'Alpha'), ('를', 'Noun'), ('열심히', 'Adverb'), ('공부', 'Noun'), ('하고', 'Josa'), (',', 'Punctuation'), ('취업', 'Noun'), ('에', 'Josa'), ('성공합시다', 'Adjective')]\n","Okt :  ['를', '공부', '취업']\n"]}]},{"cell_type":"code","source":["print(\"kkma : \", kkma.morphs(\"NLP를 열심히 공부하고, 취업에 성공합시다\"))\n","print(\"kkma : \", kkma.pos(\"NLP를 열심히 공부하고, 취업에 성공합시다\"))\n","print(\"kkma : \", kkma.nouns(\"NLP를 열심히 공부하고, 취업에 성공합시다\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffxLtpXEIqK0","outputId":"653082e6-956f-4b73-92e6-7c85c6b490b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["kkma :  ['NLP', '를', '열심히', '공부', '하', '고', ',', '취업', '에', '성공', '합', '시다']\n","kkma :  [('NLP', 'OL'), ('를', 'JKO'), ('열심히', 'MAG'), ('공부', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), (',', 'SP'), ('취업', 'NNG'), ('에', 'JKM'), ('성공', 'NNG'), ('합', 'NNG'), ('시다', 'NNG')]\n","kkma :  ['공부', '취업', '성공', '성공합시다', '합', '시다']\n"]}]},{"cell_type":"code","source":["#빈도수가 낮거나, 길이가 매우 짧은 단어는 상황에 따라 제거 고려."],"metadata":{"id":"3PFbMNiMJapj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re #정규표현식\n","text=\"Tommy's Don't And that's exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\""],"metadata":{"id":"7SJPc4iHKvDc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pat=re.compile(r'\\W*\\b\\w{1,2}\\b')"],"metadata":{"id":"NfZBdVVsK9Fi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pat에 대응되는 부분을 빈 문자열로 대체\n","pat.sub('',text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"V1dNjJWSLG1M","outputId":"09570fa4-7e2e-4b1d-a675-5c1539435d4d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tommy Don And that exactly the way with our machines order get our computer understand any text need break that word down way that our machine can understand. That where the concept tokenization Natural Language Processing (NLP) comes.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["# 불용어를 제공하는 패키지 (불용어 : 의미가 없는 단어, 조사)\n","from nltk.corpus import stopwords"],"metadata":{"id":"725uKFzNLL11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')\n","stopwords.words('english')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6SXtfXn2MFKV","outputId":"e7685552-992c-429f-ffd0-2c3e36c4234d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["['i',\n"," 'me',\n"," 'my',\n"," 'myself',\n"," 'we',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'you',\n"," \"you're\",\n"," \"you've\",\n"," \"you'll\",\n"," \"you'd\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," 'he',\n"," 'him',\n"," 'his',\n"," 'himself',\n"," 'she',\n"," \"she's\",\n"," 'her',\n"," 'hers',\n"," 'herself',\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'they',\n"," 'them',\n"," 'their',\n"," 'theirs',\n"," 'themselves',\n"," 'what',\n"," 'which',\n"," 'who',\n"," 'whom',\n"," 'this',\n"," 'that',\n"," \"that'll\",\n"," 'these',\n"," 'those',\n"," 'am',\n"," 'is',\n"," 'are',\n"," 'was',\n"," 'were',\n"," 'be',\n"," 'been',\n"," 'being',\n"," 'have',\n"," 'has',\n"," 'had',\n"," 'having',\n"," 'do',\n"," 'does',\n"," 'did',\n"," 'doing',\n"," 'a',\n"," 'an',\n"," 'the',\n"," 'and',\n"," 'but',\n"," 'if',\n"," 'or',\n"," 'because',\n"," 'as',\n"," 'until',\n"," 'while',\n"," 'of',\n"," 'at',\n"," 'by',\n"," 'for',\n"," 'with',\n"," 'about',\n"," 'against',\n"," 'between',\n"," 'into',\n"," 'through',\n"," 'during',\n"," 'before',\n"," 'after',\n"," 'above',\n"," 'below',\n"," 'to',\n"," 'from',\n"," 'up',\n"," 'down',\n"," 'in',\n"," 'out',\n"," 'on',\n"," 'off',\n"," 'over',\n"," 'under',\n"," 'again',\n"," 'further',\n"," 'then',\n"," 'once',\n"," 'here',\n"," 'there',\n"," 'when',\n"," 'where',\n"," 'why',\n"," 'how',\n"," 'all',\n"," 'any',\n"," 'both',\n"," 'each',\n"," 'few',\n"," 'more',\n"," 'most',\n"," 'other',\n"," 'some',\n"," 'such',\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'only',\n"," 'own',\n"," 'same',\n"," 'so',\n"," 'than',\n"," 'too',\n"," 'very',\n"," 's',\n"," 't',\n"," 'can',\n"," 'will',\n"," 'just',\n"," 'don',\n"," \"don't\",\n"," 'should',\n"," \"should've\",\n"," 'now',\n"," 'd',\n"," 'll',\n"," 'm',\n"," 'o',\n"," 're',\n"," 've',\n"," 'y',\n"," 'ain',\n"," 'aren',\n"," \"aren't\",\n"," 'couldn',\n"," \"couldn't\",\n"," 'didn',\n"," \"didn't\",\n"," 'doesn',\n"," \"doesn't\",\n"," 'hadn',\n"," \"hadn't\",\n"," 'hasn',\n"," \"hasn't\",\n"," 'haven',\n"," \"haven't\",\n"," 'isn',\n"," \"isn't\",\n"," 'ma',\n"," 'mightn',\n"," \"mightn't\",\n"," 'mustn',\n"," \"mustn't\",\n"," 'needn',\n"," \"needn't\",\n"," 'shan',\n"," \"shan't\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'wasn',\n"," \"wasn't\",\n"," 'weren',\n"," \"weren't\",\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\"]"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","source":["text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"tpGj2F4IMLLc","outputId":"1bcaa466-cc28-4218-f949-62ae56f63b46"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Tommy's Don't And that's exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":86}]},{"cell_type":"code","source":["len(set(stopwords.words(\"english\"))) #179\n","sw=set(stopwords.words(\"english\")) #중복방지"],"metadata":{"id":"5seZoqwPMs6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 불용어 제거단계\n","# 단어들로 분해하여 불용어리스트에 없다면 res에 추가\n","wt=word_tokenize(text)\n","res=[]\n","for w in wt:\n","    if w not in sw:\n","        res.append(w)"],"metadata":{"id":"dci4WjaZM2FL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(wt)\n","print(len(wt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mS6_WiXINQoL","outputId":"315f9a1b-ea48-4cc5-ec95-b95924c361fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tommy', \"'s\", 'Do', \"n't\", 'And', 'that', \"'s\", 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n","59\n"]}]},{"cell_type":"code","source":["print(res)\n","print(len(res))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vo5qua60N2NV","outputId":"e022aa62-b956-4271-dad8-aa78fda1510b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tommy', \"'s\", 'Do', \"n't\", 'And', \"'s\", 'exactly', 'way', 'machines', '.', 'In', 'order', 'get', 'computer', 'understand', 'text', ',', 'need', 'break', 'word', 'way', 'machine', 'understand', '.', 'That', '’', 'concept', 'tokenization', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', '.']\n","36\n"]}]},{"cell_type":"code","source":["text=\"NLP를 열심히 공부하고, 취업에 성공합시다\"\n","sw=\"를 에 고 라고 다\""],"metadata":{"id":"udQ3k0gSN9Ia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sw=sw.split(\" \")\n","sw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdrhxigHOn4p","outputId":"8caf7b39-3729-407d-cb56-f427b91dd06a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['를', '에', '고', '라고', '다']"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["wt=okt.morphs(text)\n","res=[w for w in wt if not w in sw]"],"metadata":{"id":"UG5rSIrwOuKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IoWk0zRhO4ca","outputId":"50ff0f30-0cb0-4560-fe4c-d1584686cec2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['NLP', '를', '열심히', '공부', '하고', ',', '취업', '에', '성공합시다']"]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["res"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K58p78RgPbvE","outputId":"eb4fbd2d-1cdc-4e9d-f734-11adedd01b6b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['NLP', '열심히', '공부', '하고', ',', '취업', '성공합시다']"]},"metadata":{},"execution_count":102}]},{"cell_type":"code","source":["#원핫인코딩\n","#단어 -> 정수화(index)\n","#good hi hello -> 0 1 2 => 100 010 001"],"metadata":{"id":"5hTXv2yJPeJq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text=\"\"\"\n","Tokenization is a key (and mandatory) aspect of working with text data\n","We’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\n","Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\n","And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\n","Simply put, we can’t work with text data if we don’t perform tokenization. Yes, it’s really that important!\n","\"\"\""],"metadata":{"id":"vUZ4nguYUvu7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sw=set(stopwords.words(\"english\"))"],"metadata":{"id":"AOQn_3qGfs4N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sents=sent_tokenize(text)"],"metadata":{"id":"N9ZRC-xeU-SS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sents\n","len(sents)#9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTC7mFpDVMlR","outputId":"b75ee055-12c4-4998-84dc-2bd579a4251e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":134}]},{"cell_type":"code","source":["vocab={}\n","pre_sents=[]\n","for s in sents: #문장 토큰화\n","    wt=word_tokenize(s)  #단어 토큰화 (단어 리스트)\n","    res=[]\n","    for w in wt:  # 각 단어 추출\n","        w=w.lower()\n","        if w not in sw: #불용어 리스트에 없고\n","            if len(w) > 2: # 길이가 2보다 크다면\n","                res.append(w)\n","                if w not in vocab: #vocab는 빈도를 기록하는 사전\n","                    vocab[w]=0\n","                vocab[w]+=1\n","    pre_sents.append(res) # pre_sents 는 문장들의 단어들 중 불용어 제거한 단어 리스트\n","print(pre_sents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IGaIz422VOHL","outputId":"18c4080d-d09b-41f0-cbed-c66725f75149"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['tokenization', 'key', 'mandatory', 'aspect', 'working', 'text', 'data', 'discuss', 'various', 'nuances', 'tokenization', 'including', 'handle', 'out-of-vocabulary', 'words', 'oov', 'language', 'thing', 'beauty'], ['mastering', 'new', 'language', 'scratch', 'quite', 'daunting', 'prospect'], ['ever', 'picked', 'language', 'mother', 'tongue', 'relate'], ['many', 'layers', 'peel', 'syntaxes', 'consider', 'quite', 'challenge'], ['exactly', 'way', 'machines'], ['order', 'get', 'computer', 'understand', 'text', 'need', 'break', 'word', 'way', 'machine', 'understand'], ['concept', 'tokenization', 'natural', 'language', 'processing', 'nlp', 'comes'], ['simply', 'put', 'work', 'text', 'data', 'perform', 'tokenization'], ['yes', 'really', 'important']]\n"]}]},{"cell_type":"code","source":["vocab\n","#vocab['and']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2hFEidmpYGQI","outputId":"519d2cfe-ea10-4703-db56-f8fd3c82327f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'tokenization': 4,\n"," 'key': 1,\n"," 'mandatory': 1,\n"," 'aspect': 1,\n"," 'working': 1,\n"," 'text': 3,\n"," 'data': 2,\n"," 'discuss': 1,\n"," 'various': 1,\n"," 'nuances': 1,\n"," 'including': 1,\n"," 'handle': 1,\n"," 'out-of-vocabulary': 1,\n"," 'words': 1,\n"," 'oov': 1,\n"," 'language': 4,\n"," 'thing': 1,\n"," 'beauty': 1,\n"," 'mastering': 1,\n"," 'new': 1,\n"," 'scratch': 1,\n"," 'quite': 2,\n"," 'daunting': 1,\n"," 'prospect': 1,\n"," 'ever': 1,\n"," 'picked': 1,\n"," 'mother': 1,\n"," 'tongue': 1,\n"," 'relate': 1,\n"," 'many': 1,\n"," 'layers': 1,\n"," 'peel': 1,\n"," 'syntaxes': 1,\n"," 'consider': 1,\n"," 'challenge': 1,\n"," 'exactly': 1,\n"," 'way': 2,\n"," 'machines': 1,\n"," 'order': 1,\n"," 'get': 1,\n"," 'computer': 1,\n"," 'understand': 2,\n"," 'need': 1,\n"," 'break': 1,\n"," 'word': 1,\n"," 'machine': 1,\n"," 'concept': 1,\n"," 'natural': 1,\n"," 'processing': 1,\n"," 'nlp': 1,\n"," 'comes': 1,\n"," 'simply': 1,\n"," 'put': 1,\n"," 'work': 1,\n"," 'perform': 1,\n"," 'yes': 1,\n"," 'really': 1,\n"," 'important': 1}"]},"metadata":{},"execution_count":138}]},{"cell_type":"code","source":["vocab\n","#빈도수가 높은 순서대로 정렬하여 출력\n","#[('that', 6), ...]\n","# x=[10,30,20,25]\n","# sorted(x, reverse=True)\n","\n","vocab.items()\n","vs=sorted(vocab.items(), key=lambda x:x[1], reverse=True) #x에는 ('tokenization', 4).. 앞의 매개변수 vocab.items 전달\n","vs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_cjCAkcY8dn","outputId":"d050ec62-1a4c-4656-df15-b6c0e212aff8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('tokenization', 4),\n"," ('language', 4),\n"," ('text', 3),\n"," ('data', 2),\n"," ('quite', 2),\n"," ('way', 2),\n"," ('understand', 2),\n"," ('key', 1),\n"," ('mandatory', 1),\n"," ('aspect', 1),\n"," ('working', 1),\n"," ('discuss', 1),\n"," ('various', 1),\n"," ('nuances', 1),\n"," ('including', 1),\n"," ('handle', 1),\n"," ('out-of-vocabulary', 1),\n"," ('words', 1),\n"," ('oov', 1),\n"," ('thing', 1),\n"," ('beauty', 1),\n"," ('mastering', 1),\n"," ('new', 1),\n"," ('scratch', 1),\n"," ('daunting', 1),\n"," ('prospect', 1),\n"," ('ever', 1),\n"," ('picked', 1),\n"," ('mother', 1),\n"," ('tongue', 1),\n"," ('relate', 1),\n"," ('many', 1),\n"," ('layers', 1),\n"," ('peel', 1),\n"," ('syntaxes', 1),\n"," ('consider', 1),\n"," ('challenge', 1),\n"," ('exactly', 1),\n"," ('machines', 1),\n"," ('order', 1),\n"," ('get', 1),\n"," ('computer', 1),\n"," ('need', 1),\n"," ('break', 1),\n"," ('word', 1),\n"," ('machine', 1),\n"," ('concept', 1),\n"," ('natural', 1),\n"," ('processing', 1),\n"," ('nlp', 1),\n"," ('comes', 1),\n"," ('simply', 1),\n"," ('put', 1),\n"," ('work', 1),\n"," ('perform', 1),\n"," ('yes', 1),\n"," ('really', 1),\n"," ('important', 1)]"]},"metadata":{},"execution_count":139}]},{"cell_type":"code","source":["#빈도수가 2이상인 단어들에 대해서 index를(1번~) 부여\n","word_index={}\n","a=0\n","for w, f in vs:\n","    if f>=2:\n","        a+=1\n","        word_index[w]=a\n","word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4V0pBRUZYQ5","outputId":"7fd76734-f4a8-46e4-b54b-27378e55600f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'tokenization': 1,\n"," 'language': 2,\n"," 'text': 3,\n"," 'data': 4,\n"," 'quite': 5,\n"," 'way': 6,\n"," 'understand': 7}"]},"metadata":{},"execution_count":140}]},{"cell_type":"code","source":["sents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ndsFL6kgcI8R","outputId":"f046c567-d899-48cf-d36a-4f54f0b8f12d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\nTokenization is a key (and mandatory) aspect of working with text data\\nWe’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\\nLanguage is a thing of beauty.',\n"," 'But mastering a new language from scratch is quite a daunting prospect.',\n"," 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n"," 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.',\n"," 'And that’s exactly the way with our machines.',\n"," 'In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand.',\n"," 'That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.',\n"," 'Simply put, we can’t work with text data if we don’t perform tokenization.',\n"," 'Yes, it’s really that important!']"]},"metadata":{},"execution_count":131}]},{"cell_type":"code","source":["word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7ZBIwGpibSk","outputId":"f7d4f621-c604-40f1-b717-16d2cae2054e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'tokenization': 1,\n"," 'language': 2,\n"," 'text': 3,\n"," 'data': 4,\n"," 'quite': 5,\n"," 'way': 6,\n"," 'understand': 7}"]},"metadata":{},"execution_count":142}]},{"cell_type":"code","source":["word_index['OOV']=len(word_index)+1 #Out Of Vocab"],"metadata":{"id":"spvjXPZxjOA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc_sents=[]\n","for s in pre_sents:\n","    enc_sent=[]\n","    for w in s:\n","        try:\n","            enc_sent.append(word_index[w])\n","        except KeyError:\n","            enc_sent.append(word_index['OOV'])\n","    enc_sents.append(enc_sent)\n","enc_sents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UJA962tscnuH","outputId":"591d61bb-9471-4b6b-8ec2-25755f6b720a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 8, 8, 8, 8, 3, 4, 8, 8, 8, 1, 8, 8, 8, 8, 8, 2, 8, 8],\n"," [8, 8, 2, 8, 5, 8, 8],\n"," [8, 8, 2, 8, 8, 8],\n"," [8, 8, 8, 8, 8, 5, 8],\n"," [8, 6, 8],\n"," [8, 8, 8, 7, 3, 8, 8, 8, 6, 8, 7],\n"," [8, 1, 8, 2, 8, 8, 8],\n"," [8, 8, 8, 3, 4, 8, 1],\n"," [8, 8, 8]]"]},"metadata":{},"execution_count":146}]},{"cell_type":"code","source":["# 데이터수집(크롤링)도구:selenium, beautifulsoup4...\n","# 시각화도구: matplotlib,plotly,tableau,seaborn,folium\n","# 데이터분석도구:numpy,pandas\n","# 머신러닝도구:scikit-learn\n","# 딥러닝 프레임웍:텐서플로우(케라스), 파이토치, 까페, ..."],"metadata":{"id":"YIktdEPSho_n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer"],"metadata":{"id":"Lh_sDOEPluAV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pre_sents\n","tok=Tokenizer()\n","tok.fit_on_texts(pre_sents) #모든 단어에 대해 번호를 부여"],"metadata":{"id":"EREaawSDl3Wt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok.word_index\n","tok.word_counts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1yvFscLSmLMF","outputId":"c7538c1a-1dcb-43b3-8e75-1b8fa6ffaac0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('tokenization', 4),\n","             ('key', 1),\n","             ('mandatory', 1),\n","             ('aspect', 1),\n","             ('working', 1),\n","             ('text', 3),\n","             ('data', 2),\n","             ('discuss', 1),\n","             ('various', 1),\n","             ('nuances', 1),\n","             ('including', 1),\n","             ('handle', 1),\n","             ('out-of-vocabulary', 1),\n","             ('words', 1),\n","             ('oov', 1),\n","             ('language', 4),\n","             ('thing', 1),\n","             ('beauty', 1),\n","             ('mastering', 1),\n","             ('new', 1),\n","             ('scratch', 1),\n","             ('quite', 2),\n","             ('daunting', 1),\n","             ('prospect', 1),\n","             ('ever', 1),\n","             ('picked', 1),\n","             ('mother', 1),\n","             ('tongue', 1),\n","             ('relate', 1),\n","             ('many', 1),\n","             ('layers', 1),\n","             ('peel', 1),\n","             ('syntaxes', 1),\n","             ('consider', 1),\n","             ('challenge', 1),\n","             ('exactly', 1),\n","             ('way', 2),\n","             ('machines', 1),\n","             ('order', 1),\n","             ('get', 1),\n","             ('computer', 1),\n","             ('understand', 2),\n","             ('need', 1),\n","             ('break', 1),\n","             ('word', 1),\n","             ('machine', 1),\n","             ('concept', 1),\n","             ('natural', 1),\n","             ('processing', 1),\n","             ('nlp', 1),\n","             ('comes', 1),\n","             ('simply', 1),\n","             ('put', 1),\n","             ('work', 1),\n","             ('perform', 1),\n","             ('yes', 1),\n","             ('really', 1),\n","             ('important', 1)])"]},"metadata":{},"execution_count":153}]},{"cell_type":"code","source":["#패딩: 단어(PAD)에 대해 0으로 설정하여 길이를 맞춤"],"metadata":{"id":"lp6YZBktmyUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pre_sentences = [['driver', 'person'], ['driver', 'good', 'person'], ['driver', 'huge', 'person'], ['knew', 'bad'], ['bad', 'kept', 'huge', 'bad'], ['huge', 'bad']]"],"metadata":{"id":"qkhDGUJSnw9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#단어 정수 인코딩\n","tok=Tokenizer()\n","tok.fit_on_texts(pre_sentences) #모든 단어에 대해 번호를 부여"],"metadata":{"id":"Exgm_6pwnx4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded=tok.texts_to_sequences(pre_sentences) #길이 4로 통일 (가장 긴 문장의 길이를 기준으로 패딩)\n","encoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n1lY-M3hoUK8","outputId":"6110277b-ff9d-4629-ee15-be9515b5a6ba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]"]},"metadata":{},"execution_count":158}]},{"cell_type":"code","source":["maxlen=max(len(i) for i in encoded)\n","maxlen"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugCtzikBomwz","outputId":"dabf53d9-e16a-487c-c9f6-801ce3fb8f7f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":160}]},{"cell_type":"code","source":["for s in encoded:\n","    while len(s) < maxlen:\n","        s.append(0)"],"metadata":{"id":"u0z3W8Dop1jL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-5K7GHPqNfs","outputId":"f7a5a788-173b-4ab7-ca7a-ca84ebf6a8ec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[4, 1, 0, 0]"]},"metadata":{},"execution_count":162}]},{"cell_type":"code","source":["encoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8GgmQ-mqN80","outputId":"a81fddcc-d913-4a49-b143-c36065ff4da3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 3, 0, 0],\n"," [2, 5, 3, 0],\n"," [2, 4, 3, 0],\n"," [6, 1, 0, 0],\n"," [1, 7, 4, 1],\n"," [4, 1, 0, 0]]"]},"metadata":{},"execution_count":163}]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"VtWETBk-qlUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.array(encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbgKaq_JqYNb","outputId":"e8bed571-671e-4158-ccb2-8bff7aaf9905"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 3, 0, 0],\n","       [2, 5, 3, 0],\n","       [2, 4, 3, 0],\n","       [6, 1, 0, 0],\n","       [1, 7, 4, 1],\n","       [4, 1, 0, 0]])"]},"metadata":{},"execution_count":166}]},{"cell_type":"code","source":["#사실상 가장 중요한 부분 *******\n","#케라스 도구를 이용한 패딩(많이 사용됨)"],"metadata":{"id":"Yd7zKWdAqklT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"jISVw1Xaq2zD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded=tok.texts_to_sequences(pre_sentences)\n","encoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eTFRvou_q-oj","outputId":"94ed0216-78f9-4ed6-a0c5-bf4eceb01500"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]"]},"metadata":{},"execution_count":169}]},{"cell_type":"code","source":["padded=pad_sequences(encoded, padding='post', maxlen=10) #pre가 기본값\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DzsR4HnmrOFD","outputId":"126d3746-8ae6-4329-f99a-65de3ad63064"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n","       [2, 5, 3, 0, 0, 0, 0, 0, 0, 0],\n","       [2, 4, 3, 0, 0, 0, 0, 0, 0, 0],\n","       [6, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","       [1, 7, 4, 1, 0, 0, 0, 0, 0, 0],\n","       [4, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"]},"metadata":{},"execution_count":172}]},{"cell_type":"code","source":["padded=pad_sequences(encoded, maxlen=4)\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fyF3bbZrXyC","outputId":"a95eec36-792b-412e-d01f-20a972871732"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 2, 3],\n","       [0, 2, 5, 3],\n","       [0, 2, 4, 3],\n","       [0, 0, 6, 1],\n","       [1, 7, 4, 1],\n","       [0, 0, 4, 1]], dtype=int32)"]},"metadata":{},"execution_count":179}]},{"cell_type":"code","source":["padded=pad_sequences(encoded, maxlen=2, truncating='post')\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uiBxOCe-sCO5","outputId":"15582823-ef67-4618-bb6b-5a169de40ad9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 3],\n","       [2, 5],\n","       [2, 4],\n","       [6, 1],\n","       [1, 7],\n","       [4, 1]], dtype=int32)"]},"metadata":{},"execution_count":181}]},{"cell_type":"code","source":["padded=pad_sequences(encoded, maxlen=2, truncating='pre')\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mTcOp1cJseFb","outputId":"b4e86f2f-9a72-44cb-a878-428acb41844b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 3],\n","       [5, 3],\n","       [4, 3],\n","       [6, 1],\n","       [4, 1],\n","       [4, 1]], dtype=int32)"]},"metadata":{},"execution_count":182}]},{"cell_type":"code","source":["#원핫인코딩\n","#단어 종류 : 10개 -> 인코딩 => 머신러닝/딥러닝 언어모델링(분류/생성/이해...)\n","#1~10번까지 번호 부여(1:sky, 2:computer, 3:pencil, ...)\n","\n","# sky:      1000000000\n","# computer: 0100000000\n","# ..."],"metadata":{"id":"KHPAgL4SsstT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","단어의 종류가 10만개\n","원핫인코딩하면 각 단어는 10만차원 벡터공간에 임베딩\n"," sky: 1000000000...00000(원핫벡터) -> 차원 감소 -> [1.7 -3.5 0.3 0.1 0.2] 5차원 벡터공간 임베딩(밀집벡터)\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"emKVUlMrxL-5","outputId":"fc8fdd62-31d1-443e-d614-71f66388dae6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n단어의 종류가 10만개\\n원핫인코딩하면 각 단어는 10만차원 벡터공간에 임베딩\\n sky: 1000000000...00000(원핫벡터) -> 차원 감소 -> [1.7 -3.5 0.3 0.1 0.2] 5차원 벡터공간 임베딩(밀집벡터)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":184}]},{"cell_type":"code","source":["tokens=okt.morphs(\"자연어처리 공부를 합니다\")\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PnbUjTNSyREZ","outputId":"c862095d-d1d8-49df-f2ab-8a1f20b28812"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['자연어', '처리', '공부', '를', '합니다']"]},"metadata":{},"execution_count":185}]},{"cell_type":"code","source":["# 10000(자연어)\n","# 01000(처리)"],"metadata":{"id":"jgfhRyj7yfGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#코퍼스(수집한 문서 전체)\n","text=\"점심 메뉴로 소고기볶음밥 먹었습니다. 소고기볶음밥 너무 맛있어요. 또 먹을래요.\""],"metadata":{"id":"72GUALdwy5dZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok=Tokenizer()\n","tok.fit_on_texts([text]) #모든 단어에 대해 번호를 부여"],"metadata":{"id":"dVrd_BZlzCDB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok.word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3x5IaWL8zKGB","outputId":"804c6702-8823-4f54-e35c-9e60e3f5c530"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'소고기볶음밥': 1,\n"," '점심': 2,\n"," '메뉴로': 3,\n"," '먹었습니다': 4,\n"," '너무': 5,\n"," '맛있어요': 6,\n"," '또': 7,\n"," '먹을래요': 8}"]},"metadata":{},"execution_count":192}]},{"cell_type":"code","source":["test=\"내일 메뉴로 소고기볶음밥 또 나왔으면 좋겠다.\""],"metadata":{"id":"iF80HGYezNCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded=tok.texts_to_sequences([test])[0]  # 이차원 리스트에서 추출\n","encoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMaGyQNtzg2C","outputId":"31feaa77-4df5-4219-dbe5-73e5644efee2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[3, 1, 7]"]},"metadata":{},"execution_count":196}]},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","# 이진화 원핫인코딩"],"metadata":{"id":"drKqKVURzpfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["o_v=to_categorical(encoded)"],"metadata":{"id":"YHHjbkyO0kCY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["o_v"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzQ48f4E0pQZ","outputId":"c251d474-be06-488d-e04f-d1aaa556718d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 1., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"]},"metadata":{},"execution_count":199}]},{"cell_type":"code","source":["# 홍길동 영화 -> 추천?\n","# -영화의 줄거리를 기반으로 추천(tfidf -> consine sim -> 추천)"],"metadata":{"id":"ws3CHBVa0qIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re"],"metadata":{"id":"pku1RQPc2u5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text=\"\"\"\n","[앵커]\n","\n","오후가 되면서 비구름이 약해졌지만 내일 새벽부터 다시 전국에 비가 내리겠습니다.\n","\n","특히, 호남지방은 내일까지 많게는 최대 120mm의 비가 예보됐습니다.\n","\n","기상센터 연결해 자세한 장맛비 상황 알아보겠습니다.\n","\n","김규리 캐스터! 현재, 비는 어느 지역에 집중되고 있나요?\n","\n","[답변]\n","\n","네, 현재는 남해안과 제주에 시간당 20mm의 세찬 비가 내리고 있습니다.\n","\n","이에 경남과 전남 남해안, 제주에 호우주의보가 발효 중입니다.\n","\n","어젯밤사이에는 광주, 전남에 폭우가 내렸습니다.\n","\n","어제부터 오늘까지 전남 무안에 255mm의 많은 비가 쏟아졌고요.\n","\n","광주도 145mm의 큰 비가 집중됐습니다.\n","\n","현재 보이는 화면은 광주 황룡강의 모습입니다.\n","\n","폭우가 집중되며 오늘 새벽 홍수주의보가 내려졌습니다.\n","\n","현재는, 광주·전남 등 내륙 지역은 대부분 소강상태를 보이고 있습니다.\n","\n","지금도 흙탕물에 나무가 잠겨 있지만 홍수주의보 기준인 5.5m 아래로 낮아지고 있습니다.\n","\n","[앵커]\n","\n","내일 새벽부터 다시 빗줄기가 강해질텐데, 얼마나 더 오는건가요?\n","\"\"\""],"metadata":{"id":"xq3lvAX-3aKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(re.findall('\\w+mm', text))\n","#re.match(패턴, 문자열)\n","#a#문자열 내에 패턴이 존재합니까?\n","#패턴이 있다면 매치 결과가 출력\n","#없다면 None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AAK48Gn52vQI","outputId":"f4e50ce4-f4b9-4f45-877e-0d392827f2e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['120mm', '20mm', '255mm', '145mm']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"nyMie_PL2xaq"},"execution_count":null,"outputs":[]}]}