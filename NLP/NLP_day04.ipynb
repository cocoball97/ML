{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# 손실함수(Loss function)와 비용함수(cost function)의 차이\n","# 보통 동일한 의미로 사용되지만 엄밀히 구분하자면,\n","\n","# 손실함수(Loss function)는 샘플 하나에 대한 손실을 의미하며\n","\n","# 비용함수(cost function)는 훈련세트에 있는 모든 샘플에 대한 손실함수의 합을 의미한다.\n"],"metadata":{"id":"lkzUyynaTx6a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 순전파 : 기울기(gradient)를 사용하는 것이 아닌, 각 층에서의 가중치와 활성화 함수에 의해 입력 데이터가 변환되는 과정을 수행합니다. 가중치는  다양한 데이터에 적응할 수 있도록 무작위 초기화  sigmoid : 0~0.25, tanh:-1~1\n","# 역전파 : 손실 함수의 기울기(gradient)를 계산하고, 이를 사용하여 모델의 가중치를 업데이트하는 과정을 말합니다. 손실 함수를 통해 모델의 예측과 실제 타깃 값 사이의 차이를 계산합니다. (최적화 알고리즘 사용 x)\n","# 각 층의 가중치에 대한 손실 함수의 기울기를 계산하고, 이를 이용하여 경사하강법 등의 최적화 알고리즘을 사용하여 가중치를 업데이트합니다. 역전파 과정에서는 기울기를 미분하고 연쇄 법칙을 이용하여 값들을 계산합니다."],"metadata":{"id":"pMGMD1WLTfFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qjgqXOLgT42L"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqVlaEhikNWk"},"outputs":[],"source":["#연습문제\n","#1. 전처리(불용어 제거)\n","#2. 모델 구조 변경 및 성능 개선\n","#3. 소설 책(토) 중 일부를 발췌하여 모델링 연습 (선택사항, 형태소분석기?)\n","\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, SimpleRNN, LSTM\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import pandas as pd\n","import numpy as np\n","from string import punctuation"]},{"cell_type":"code","source":["texts=['자연어 처리 알고리즘', '자연어 처리 방법',\n","       '자연어 NLP 알고리즘 알고리즘',\n","       '자연어 처리 전문가']"],"metadata":{"id":"bhOK23dfx2yB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok=Tokenizer()#클래스(설계도), 객체(건물)\n","tok.fit_on_texts(texts)"],"metadata":{"id":"uhAuPyHJyJwn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok.index_word  # index_word 가 1부터 시작되지만 matrix에서는 0부터 시작"],"metadata":{"id":"QLqTAdxnySNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok.texts_to_matrix(texts) # default : mode='binary'\n","tok.texts_to_matrix(texts, mode='binary')\n","tok.texts_to_matrix(texts, mode='count')\n","tok.texts_to_matrix(texts, mode='tfidf')\n","tok.texts_to_matrix(texts, mode='freq')"],"metadata":{"id":"zjKCepx3zQ5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/NLP/dataset/NYT_2018.csv\")\n","df\n","df.headline.isnull().sum()"],"metadata":{"id":"X9J5aICBzhRn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["headline=[]\n","headline.extend(list(df.headline.values))  # 비슷하지만 append는 원소 1개/ extend는 iteratble 모든 항목을 추가한다."],"metadata":{"id":"_GqCpGIF5tTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["headline"],"metadata":{"id":"iDm-2irU6RVX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(headline) #1324"],"metadata":{"id":"7UaASQi76pqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sum(df.headline=='Unknown')"],"metadata":{"id":"yKzXqKE07iPE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len([w for w in headline if w=='Unknown'])"],"metadata":{"id":"LRrndVWC7vss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["headline=[w for w in headline if w!='Unknown']\n","len(headline)"],"metadata":{"id":"d4aoRSd-74v0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# headline에 저장된 뉴스 기사 제목으로 다음 단어를 생성하는 LSTM\n","# 기반 모델 설계\n","# 동작 예\n","# 입력 : I, 생성하고자 하는 단어의 갯수\n","# 출력 : I was ...완성"],"metadata":{"id":"gChdjIp68708"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pre_func(title):\n","    #소문자 변환\n","    res=''.join(w.lower() for w in title if w not in punctuation)\n","    return res"],"metadata":{"id":"JeOiDmvu-awt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["punctuation"],"metadata":{"id":"ySafKn83AmDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pre_headline=[pre_func(x) for x in headline]\n","\"\"\"\n","동작순서\n","x in headline -> pre_func(x) -> pre_func(title) -> for w in title\n","->if w not in punctuation -> w.lower() -> ''.join -> 소문자로 변환된 단어들이 연결됨\n","-> return res -> pre_headline리스트의 요소로 저장됨\n","\"\"\""],"metadata":{"id":"23FC33lG9peM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pre_headline"],"metadata":{"id":"MD4ovz8z_V-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok=Tokenizer()"],"metadata":{"id":"6--sEha3_XR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok.fit_on_texts(pre_headline)"],"metadata":{"id":"Eflo16MvBY6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok.index_word\n","len(tok.index_word)"],"metadata":{"id":"vVzMX5aTBd89"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size=len(tok.index_word)+1 #3620"],"metadata":{"id":"udy2uaXPBiZ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pre_headline"],"metadata":{"id":"UNBP6bCXGpYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences=[]\n","for s in pre_headline:\n","    print(tok.texts_to_sequences([s])[0]) #각 문장별 인코딩\n"],"metadata":{"id":"HXGJWvVAoTCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences=[]\n","for s in pre_headline:\n","    # print(s)\n","    print(tok.texts_to_sequences([s])[0]) #각 문장별 인코딩      2차원 배열내의 정보 가져오기 위해 [0]\n","    enc=tok.texts_to_sequences([s])[0]\n","    # 질문!! 반복문이 i가 다시 0부터 할당된느거아님?\n","\n","\n","\n","    for i in range(1, len(enc)):\n","        seq=enc[:i+1]\n","        sequences.append(seq)"],"metadata":{"id":"yIOZCxVbB3Qr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences[:15]\n","\n","# ex)lstm 모델로 단어를 예측하도록 설계합니다.\n","# 입력                                    출력\n","# ------------------------------------------------------\n","# lstm                                    모델로\n","# lstm 모델로                             단어를\n","# ...\n","# lstm 모델로 단어를 예측하도록           설계합니다\n","\n","\n"],"metadata":{"id":"zRn0U-J4CF3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# key, value 교환\n","idx2word={}\n","type(tok.word_index)\n","for k, v in tok.word_index.items():\n","    idx2word[v]=k"],"metadata":{"id":"T4IHSLxjjrR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx2word"],"metadata":{"id":"SQYYrkG9jrOA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(max(len(i) for i in sequences)) #24단어 문장이 가장 길다\n","ml=max(len(i) for i in sequences)"],"metadata":{"id":"tLos_66ajrKB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#[len(i) for i in sequences]"],"metadata":{"id":"H8clMHUvjrF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences[1]"],"metadata":{"id":"KUTT-ZizjrCA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences=pad_sequences(sequences, maxlen=ml, padding='pre')"],"metadata":{"id":"r14SwSFpjq-o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences[1]"],"metadata":{"id":"n1G1IT2Kjq7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences[8]"],"metadata":{"id":"Q-3tFPxXjq3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#sequences를 x,y로 분리하여 저장\n","# x = [i[:-1] for i in sequences ]\n","# y = [i[-1] for i in sequences ]\n","sequences=np.array(sequences)\n","x=sequences[:,:-1]\n","y=sequences[:,-1]"],"metadata":{"id":"AJrSOLTdjqzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape\n","y.shape"],"metadata":{"id":"OSCJxGlrorNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences.shape #(7809, 24)"],"metadata":{"id":"Ju7EatLDjqwZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"id":"HU5co2lhjqsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y=to_categorical(y, vocab_size) # 원핫 인코딩  vocab_size : 인코딩 개수"],"metadata":{"id":"B1jOymo-jqpJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.shape"],"metadata":{"id":"4LpKlGq53W4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(sequences[0])"],"metadata":{"id":"iFbxolvbjqlg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Embedding() : Embedding()은 단어를 밀집 벡터로 만드는 역할을 합니다.\n","# 단어 임베딩은 차원을 효과적으로 압축하여 희소성을 줄이고, 단어 간의 의미적 유사성을 파악하기 위해 많이 사용되는 기술입니다.\n","model=Sequential()\n","model.add(Embedding(vocab_size, 10))#3620차원을 -> 10차원으로 임베딩\n","model.add(LSTM(128,return_sequences=True))  # return_sequences=true\n","model.add(LSTM(64,return_sequences=False))   # 마지막 LSTM모델에서는  return_sequences=false\n","model.add(Dense(vocab_size, activation='softmax'))"],"metadata":{"id":"MNg5QYmKjqhy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"r-Q5yUq7jqdw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss=\"categorical_crossentropy\",\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","model.fit(x,y,epochs=200, verbose=2)"],"metadata":{"id":"Pf17eLA5jqaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 입력 -> '나는' + 단어 10개 예상\n","\n","# 나는 -> 모델 -> 나는 지금 -> 모델 -> 나는 지금 집에 -> 모델 ->...\n","# ->나는 지금 집에 있는데 TV를 시청하고 있다...\n"],"metadata":{"id":"WfDqHDpHjqP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_sent(model, tok, c_word, n):  #tok : 인코딩해놓은 단어사전\n","    pred_sent=''\n","    for _ in range(n):\n","      enc=tok.texts_to_sequences([c_word])[0]  # c_world 에 해당하는 인덱싱 배열\n","      enc=pad_sequences([enc], maxlen=ml, padding='pre')  #pad_sequences 는 항상 2차원 배열을 입력받음\n","      res=model.predict(enc)  #enc:리스트형태\n","      res=np.argmax(res)\n","      for w, i in tok.word_index.items():\n","          if i==res:\n","              break\n","      print(\"예측단어:\",w)\n","      c_word=c_word+ \" \" + w    #  The=>  The new\n","    pred_sent=c_word\n","    return pred_sent\n","\n"],"metadata":{"id":"GrZ7PXPZjp-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc"],"metadata":{"id":"w68juUQF1nWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gen_sent(model, tok, 'The', 10)"],"metadata":{"id":"c5_Q7ZpX4WtU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 참고사이트\n","# word2vec.kr"],"metadata":{"id":"w-uyQi1RIOhy"},"execution_count":null,"outputs":[]}]}